{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe99436-d477-4f99-bfb1-d47e3a775ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:40:35.406632Z",
     "iopub.status.busy": "2025-10-01T06:40:35.405927Z",
     "iopub.status.idle": "2025-10-01T06:40:35.411682Z",
     "shell.execute_reply": "2025-10-01T06:40:35.410850Z",
     "shell.execute_reply.started": "2025-10-01T06:40:35.406599Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import psutil\n",
    "import random\n",
    "import time\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from shuffle_worker import process_batch_worker\n",
    "from typing import List, Set, Dict, Tuple, Optional, NamedTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1761ab-49e4-4fa0-9c81-7d2faa230a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:40:35.413631Z",
     "iopub.status.busy": "2025-10-01T06:40:35.413011Z",
     "iopub.status.idle": "2025-10-01T06:40:35.417358Z",
     "shell.execute_reply": "2025-10-01T06:40:35.416470Z",
     "shell.execute_reply.started": "2025-10-01T06:40:35.413605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc85891-bd82-4d57-8f34-50cb9b14016e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:49:44.233080Z",
     "iopub.status.busy": "2025-10-01T06:49:44.232606Z",
     "iopub.status.idle": "2025-10-01T06:49:44.268148Z",
     "shell.execute_reply": "2025-10-01T06:49:44.267118Z",
     "shell.execute_reply.started": "2025-10-01T06:49:44.233042Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UniformityTestResult:\n",
    "    \"\"\"Results from uniformity testing\"\"\"\n",
    "    chi2_statistic: float\n",
    "    p_value: float\n",
    "    degrees_of_freedom: int\n",
    "    n_bins: int\n",
    "    sample_size: int\n",
    "    is_uniform: bool\n",
    "    uniformity_score: float\n",
    "    observed_frequencies: List[int]\n",
    "    expected_frequencies: List[float]\n",
    "    bin_edges: List[float]\n",
    "    config_idx_range: Tuple[int, int]\n",
    "\n",
    "@dataclass\n",
    "class ChunkTestResult:\n",
    "    \"\"\"Results from testing a single chunk\"\"\"\n",
    "    chunk_file: str\n",
    "    chunk_index: int\n",
    "    total_rows: int\n",
    "    sample_size: int\n",
    "    result: UniformityTestResult\n",
    "    is_uniform: bool\n",
    "    uniformity_score: float\n",
    "\n",
    "class ConfigIdxUniformityTester:\n",
    "    def __init__(self, \n",
    "                 config_idx_min: int,\n",
    "                 config_idx_max: int,\n",
    "                 alpha: float = 0.05\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize config_idx uniformity tester\n",
    "        \n",
    "        Args:\n",
    "            config_idx_min: Minimum config_idx value across all files\n",
    "            config_idx_max: Maximum config_idx value across all files\n",
    "            alpha: Significance level for hypothesis test\n",
    "        \"\"\"\n",
    "        self.config_idx_min = config_idx_min\n",
    "        self.config_idx_max = config_idx_max\n",
    "        self.alpha = alpha\n",
    "        self.n_categories = config_idx_max - config_idx_min + 1\n",
    "                \n",
    "        logger.info(f\"Initialized config_idx uniformity tester\")\n",
    "        logger.info(f\"Range: [{config_idx_min}, {config_idx_max}], Categories: {self.n_categories}, Alpha: {alpha}\")\n",
    "        \n",
    "    def _test_discrete_uniformity(self, values: np.ndarray) -> UniformityTestResult:\n",
    "        \"\"\"\n",
    "        Test uniformity for config_idx\n",
    "        \n",
    "        Args:\n",
    "            values: Array of config_idx values\n",
    "            \n",
    "        Returns:\n",
    "            UniformityTestResult\n",
    "        \"\"\"\n",
    "        # Count occurrences of each value\n",
    "        unique_vals, counts = np.unique(values, return_counts=True)\n",
    "        \n",
    "        # Create full frequency array (including zero counts)\n",
    "        observed_freq = np.zeros(self.n_categories, dtype=int)\n",
    "        for val, count in zip(unique_vals, counts):\n",
    "            if self.config_idx_min <= val <= self.config_idx_max:\n",
    "                observed_freq[val - self.config_idx_min] = count\n",
    "        \n",
    "        # Expected frequencies (uniform distribution)\n",
    "        total_samples = len(values)\n",
    "        expected_freq = np.full(self.n_categories, total_samples / self.n_categories)\n",
    "        \n",
    "        # Chi-squared test\n",
    "        chi2_stat, p_value = stats.chisquare(observed_freq, expected_freq)\n",
    "        \n",
    "        # Results\n",
    "        is_uniform = p_value > self.alpha\n",
    "        \n",
    "        # Uniformity score (normalized, 1.0 = perfectly uniform)\n",
    "        max_possible_chi2 = total_samples * (self.n_categories - 1)\n",
    "        uniformity_score = max(0.0, 1.0 - (chi2_stat / max_possible_chi2))\n",
    "        \n",
    "        # Create bin edges for discrete values\n",
    "        bin_edges = list(range(self.config_idx_min, self.config_idx_max + 2))\n",
    "        \n",
    "        logger.debug(f\"config_idx: χ²={chi2_stat:.4f}, p={p_value:.6f}, \"\n",
    "                    f\"uniform={is_uniform}, score={uniformity_score:.4f}\")\n",
    "        \n",
    "        return UniformityTestResult(\n",
    "            chi2_statistic=chi2_stat,\n",
    "            p_value=p_value,\n",
    "            degrees_of_freedom=self.n_categories - 1,\n",
    "            n_bins=self.n_categories,\n",
    "            sample_size=total_samples,\n",
    "            is_uniform=is_uniform,\n",
    "            uniformity_score=uniformity_score,\n",
    "            observed_frequencies=observed_freq.tolist(),\n",
    "            expected_frequencies=expected_freq.tolist(),\n",
    "            bin_edges=bin_edges,\n",
    "            config_idx_range=(self.config_idx_min, self.config_idx_max)\n",
    "        )\n",
    "    \n",
    "    def _load_and_sample_single_chunk(self, chunk_file: str, \n",
    "                                    sample_size: Optional[int] = None, \n",
    "                                    seed: int = 42) -> pl.DataFrame:\n",
    "        \"\"\"Load a single chunk file and optionally sample from it\"\"\"\n",
    "        chunk_path = Path(chunk_file)\n",
    "        if not chunk_path.exists():\n",
    "            raise FileNotFoundError(f\"Chunk file does not exist: {chunk_file}\")\n",
    "        \n",
    "        # Load the chunk\n",
    "        chunk_df = pl.read_parquet(chunk_path)\n",
    "        \n",
    "        # Validate that config_idx column exists\n",
    "        if 'config_idx' not in chunk_df.columns:\n",
    "            raise ValueError(f\"Missing config_idx column in {chunk_file}\")\n",
    "        \n",
    "        total_rows = len(chunk_df)\n",
    "        \n",
    "        # Sample if requested and if chunk is larger than sample size\n",
    "        if sample_size is not None and total_rows > sample_size:\n",
    "            logger.debug(f\"Sampling {sample_size:,} rows from {total_rows:,} total rows in {chunk_path.name}\")\n",
    "            try:\n",
    "                sampled_df = chunk_df.sample(n=sample_size, seed=seed)\n",
    "            except:\n",
    "                # Fallback: manual sampling with indices\n",
    "                np.random.seed(seed)\n",
    "                indices = np.random.choice(total_rows, size=sample_size, replace=False)\n",
    "                sampled_df = chunk_df[sorted(indices)]\n",
    "        else:\n",
    "            logger.debug(f\"Using all {total_rows:,} rows from {chunk_path.name}\")\n",
    "            sampled_df = chunk_df\n",
    "        \n",
    "        return sampled_df\n",
    "    \n",
    "    def _test_single_chunk(self, chunk_file: str, chunk_index: int, \n",
    "                          sample_size: Optional[int] = None, seed: int = 42) -> ChunkTestResult:\n",
    "        \"\"\"Test uniformity for config_idx in a single chunk\"\"\"\n",
    "        chunk_path = Path(chunk_file)\n",
    "        logger.info(f\"Testing chunk {chunk_index + 1}: {chunk_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load chunk data\n",
    "            chunk_df = self._load_and_sample_single_chunk(chunk_file, sample_size, seed + chunk_index)\n",
    "            total_rows = len(chunk_df)\n",
    "            actual_sample_size = len(chunk_df)\n",
    "                                    \n",
    "            config_idx_values = chunk_df.select('config_idx').to_numpy().flatten().astype(int)\n",
    "            \n",
    "            # Test uniformity\n",
    "            result = self._test_discrete_uniformity(config_idx_values)\n",
    "            \n",
    "            logger.info(f\"  Chunk {chunk_index + 1} results: uniform={result.is_uniform}, score={result.uniformity_score:.4f}\")\n",
    "            \n",
    "            return ChunkTestResult(\n",
    "                chunk_file=str(chunk_path),\n",
    "                chunk_index=chunk_index,\n",
    "                total_rows=total_rows,\n",
    "                sample_size=actual_sample_size,\n",
    "                result=result,\n",
    "                is_uniform=result.is_uniform,\n",
    "                uniformity_score=result.uniformity_score\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to test chunk {chunk_index + 1} ({chunk_path.name}): {e}\")\n",
    "            # Return dummy result on error\n",
    "            return ChunkTestResult(\n",
    "                chunk_file=str(chunk_path),\n",
    "                chunk_index=chunk_index,\n",
    "                total_rows=0,\n",
    "                sample_size=0,\n",
    "                result=None,\n",
    "                is_uniform=False,\n",
    "                uniformity_score=0.0\n",
    "            )\n",
    "    \n",
    "    def test_chunks_uniformity(self, chunk_files: List[str], sample_size_per_chunk: Optional[int] = None, \n",
    "                             seed: int = 42) -> List[ChunkTestResult]:\n",
    "        \"\"\"Test uniformity of config_idx for each chunk\"\"\"\n",
    "        logger.info(f\"\\n{'='*80}\")\n",
    "        logger.info(f\"CONFIG_IDX UNIFORMITY TESTING\")\n",
    "        logger.info(f\"{'='*80}\")\n",
    "        logger.info(f\"Testing {len(chunk_files)} chunks\")\n",
    "        logger.info(f\"config_idx range: [{self.config_idx_min}, {self.config_idx_max}]\")\n",
    "        if sample_size_per_chunk:\n",
    "            logger.info(f\"Sampling {sample_size_per_chunk:,} rows per chunk\")\n",
    "        else:\n",
    "            logger.info(f\"Using all rows in each chunk\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for chunk_index, chunk_file in enumerate(chunk_files):\n",
    "            chunk_result = self._test_single_chunk(\n",
    "                chunk_file, chunk_index, sample_size_per_chunk, seed)\n",
    "            results.append(chunk_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_detailed_report(self, results: List[ChunkTestResult]):\n",
    "        \"\"\"Print detailed report of config_idx uniformity tests\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"CONFIG_IDX UNIFORMITY TEST REPORT\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results to display!\")\n",
    "            return\n",
    "                \n",
    "        # Overall statistics\n",
    "        total_chunks = len(results)\n",
    "        successful_chunks = len([r for r in results if r.result is not None])\n",
    "        uniform_chunks = len([r for r in results if r.is_uniform])\n",
    "        \n",
    "        print(f\"\\nOverall Statistics:\")\n",
    "        print(f\"  Total chunks: {total_chunks}\")\n",
    "        print(f\"  Successfully tested: {successful_chunks}/{total_chunks} ({successful_chunks/total_chunks*100:.1f}%)\")\n",
    "        print(f\"  Uniform chunks: {uniform_chunks}/{successful_chunks} ({uniform_chunks/successful_chunks*100:.1f}%)\")\n",
    "        \n",
    "        if successful_chunks > 0:\n",
    "            avg_score = np.mean([r.uniformity_score for r in results if r.result is not None])\n",
    "            print(f\"  Average uniformity score: {avg_score:.4f}\")\n",
    "        \n",
    "        # Per-chunk summary (first 20 chunks)\n",
    "        print(f\"\\nPer-Chunk Summary (first 20 chunks):\")\n",
    "        print(f\"{'Chunk':<8} {'File':<30} {'Uniform':<8} {'Score':<10} {'p-value':<10}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        for result in results[:20]:\n",
    "            chunk_name = Path(result.chunk_file).name[:27] + \"...\" if len(Path(result.chunk_file).name) > 30 else Path(result.chunk_file).name\n",
    "            uniform_str = \"✓\" if result.is_uniform else \"✗\"\n",
    "            p_val = f\"{result.result.p_value:.6f}\" if result.result else \"N/A\"\n",
    "            \n",
    "            print(f\"{result.chunk_index+1:<8} {chunk_name:<30} {uniform_str:<8} \"\n",
    "                  f\"{result.uniformity_score:<10.4f} {p_val:<10}\")\n",
    "        \n",
    "        if len(results) > 20:\n",
    "            print(f\"... and {len(results) - 20} more chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b65a6c5-8d30-4473-a4fb-fbb852a6f1a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:40:35.460665Z",
     "iopub.status.busy": "2025-10-01T06:40:35.460265Z",
     "iopub.status.idle": "2025-10-01T06:40:35.465880Z",
     "shell.execute_reply": "2025-10-01T06:40:35.464930Z",
     "shell.execute_reply.started": "2025-10-01T06:40:35.460620Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShuffleConfig:\n",
    "    \"\"\"Configuration for shuffle operations\"\"\"\n",
    "    max_memory_usage_ratio: float = 0.7  # Use 70% of available memory\n",
    "    safety_margin: float = 0.9  # 90% of calculated max chunks\n",
    "    min_chunks_per_batch: int = 2\n",
    "    max_chunks_per_batch: int = 1000\n",
    "    chunk_size_variation: Tuple[float, float] = (0.8, 1.2)  # Min, max variation\n",
    "    max_workers: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019795f4-557d-4468-8b55-d210a2e109a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:57:21.291675Z",
     "iopub.status.busy": "2025-10-01T06:57:21.290964Z",
     "iopub.status.idle": "2025-10-01T06:57:21.531040Z",
     "shell.execute_reply": "2025-10-01T06:57:21.530030Z",
     "shell.execute_reply.started": "2025-10-01T06:57:21.291639Z"
    }
   },
   "outputs": [],
   "source": [
    "class MemoryAwareChunkShuffler:\n",
    "    def __init__(self, input_dir: str, output_dir: str, temp_dir: str = \"shuffle_temp\", \n",
    "                 config = None):\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.temp_dir = Path(temp_dir)\n",
    "        self.config = config or ShuffleConfig()\n",
    "        \n",
    "        # Create directories\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.temp_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Get all input chunk files\n",
    "        self.input_chunks = list(self.input_dir.glob(\"*.parquet\"))\n",
    "        if not self.input_chunks:\n",
    "            raise ValueError(f\"No parquet files found in {input_dir}\")\n",
    "        \n",
    "        logger.info(f\"Found {len(self.input_chunks)} input chunks\")\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.processed_chunks = set()\n",
    "        self.memory_stats = self._get_memory_info()\n",
    "        self.chunk_memory_usage = self._estimate_chunk_memory()\n",
    "        self.optimal_batch_size = self._calculate_optimal_batch_size()\n",
    "        self.required_passes = self._calculate_required_passes()\n",
    "        \n",
    "        # Scan for global config_idx range\n",
    "        self.config_idx_min, self.config_idx_max = self._scan_config_idx_range()\n",
    "        \n",
    "        logger.info(f\"Memory available: {self.memory_stats['available_gb']:.1f} GB\")\n",
    "        logger.info(f\"Estimated chunk memory: {self.chunk_memory_usage / 1024**2:.1f} MB\")\n",
    "        logger.info(f\"Optimal batch size: {self.optimal_batch_size} chunks\")\n",
    "        logger.info(f\"Required passes: {self.required_passes}\")\n",
    "        logger.info(f\"Global config_idx range: [{self.config_idx_min}, {self.config_idx_max}]\")\n",
    "    \n",
    "    def _get_memory_info(self) -> Dict[str, float]:\n",
    "        \"\"\"Get current memory information\"\"\"\n",
    "        memory = psutil.virtual_memory()\n",
    "        return {\n",
    "            'total_gb': memory.total / 1024**3,\n",
    "            'available_gb': memory.available / 1024**3,\n",
    "            'used_gb': memory.used / 1024**3,\n",
    "            'percent_used': memory.percent\n",
    "        }\n",
    "    \n",
    "    def _estimate_chunk_memory(self, sample_size: int = 3) -> float:\n",
    "        \"\"\"Estimate memory usage per chunk by sampling\"\"\"\n",
    "        sample_files = random.sample(self.input_chunks, min(sample_size, len(self.input_chunks)))\n",
    "        memory_usages = []\n",
    "        \n",
    "        for file_path in sample_files:\n",
    "            try:\n",
    "                # Load chunk and measure memory\n",
    "                chunk = pl.read_parquet(file_path)\n",
    "                memory_usage = chunk.estimated_size()\n",
    "                memory_usages.append(memory_usage)\n",
    "                logger.debug(f\"Chunk {file_path.name}: {memory_usage / 1024**2:.1f} MB\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not estimate memory for {file_path}: {e}\")\n",
    "        \n",
    "        if not memory_usages:\n",
    "            # Fallback estimate\n",
    "            return 100 * 1024**2  # 100MB default\n",
    "        \n",
    "        avg_memory = sum(memory_usages) / len(memory_usages)\n",
    "        # Add buffer for processing overhead\n",
    "        return avg_memory * 1.5\n",
    "    \n",
    "    def _calculate_optimal_batch_size(self) -> int:\n",
    "        \"\"\"Calculate optimal number of chunks to process in one batch\"\"\"\n",
    "        available_memory = self.memory_stats['available_gb'] * 1024**3 / self.config.max_workers\n",
    "        usable_memory = available_memory * self.config.max_memory_usage_ratio\n",
    "        \n",
    "        # Calculate theoretical max chunks\n",
    "        theoretical_max = int(usable_memory / self.chunk_memory_usage)\n",
    "        \n",
    "        # Apply safety margin and constraints\n",
    "        safe_max = int(theoretical_max * self.config.safety_margin)\n",
    "        optimal_size = max(\n",
    "            self.config.min_chunks_per_batch,\n",
    "            min(safe_max, self.config.max_chunks_per_batch)\n",
    "        )\n",
    "        \n",
    "        logger.debug(f\"Theoretical max chunks: {theoretical_max}\")\n",
    "        logger.debug(f\"Safe max chunks: {safe_max}\")\n",
    "        \n",
    "        return optimal_size\n",
    "    \n",
    "    def _calculate_required_passes(self) -> int:\n",
    "        \"\"\"Calculate number of shuffle passes needed for uniform distribution\"\"\"\n",
    "        n_chunks = len(self.input_chunks)\n",
    "        k_batch_size = self.optimal_batch_size\n",
    "        \n",
    "        if k_batch_size >= n_chunks:\n",
    "            # Can process all chunks at once\n",
    "            return 1\n",
    "        \n",
    "        # Theoretical passes based on k-way merging\n",
    "        theoretical_passes = math.ceil(math.log(n_chunks) / math.log(k_batch_size))\n",
    "        \n",
    "        # Additional mixing passes for uniformity\n",
    "        mixing_passes = max(1, math.ceil(math.log2(n_chunks) * 0.3))\n",
    "        \n",
    "        total_passes = theoretical_passes + mixing_passes\n",
    "        \n",
    "        logger.debug(f\"Theoretical passes: {theoretical_passes}\")\n",
    "        logger.debug(f\"Mixing passes: {mixing_passes}\")\n",
    "        \n",
    "        return min(total_passes, 10)  # Cap at 10 passes\n",
    "    \n",
    "    def _scan_config_idx_range(self) -> Tuple[int, int]:\n",
    "        \"\"\"Scan all input chunks to find global config_idx min and max\"\"\"\n",
    "        logger.info(\"Scanning all chunks for config_idx range...\")\n",
    "        \n",
    "        global_min = float('inf')\n",
    "        global_max = float('-inf')\n",
    "        \n",
    "        for chunk_file in self.input_chunks:\n",
    "            try:\n",
    "                # Read just the config_idx column\n",
    "                df = pl.read_parquet(chunk_file, columns=['config_idx'])\n",
    "                chunk_min = df['config_idx'].min()\n",
    "                chunk_max = df['config_idx'].max()\n",
    "                \n",
    "                global_min = min(global_min, chunk_min)\n",
    "                global_max = max(global_max, chunk_max)\n",
    "                \n",
    "                logger.debug(f\"{chunk_file.name}: config_idx range [{chunk_min}, {chunk_max}]\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not read config_idx from {chunk_file}: {e}\")\n",
    "        \n",
    "        if global_min == float('inf') or global_max == float('-inf'):\n",
    "            raise ValueError(\"Could not determine config_idx range from any chunks\")\n",
    "        \n",
    "        logger.info(f\"Global config_idx range: [{int(global_min)}, {int(global_max)}]\")\n",
    "        return int(global_min), int(global_max)\n",
    "    \n",
    "    def _select_random_chunks_for_batch(self, available_chunks: List[Path], \n",
    "                                      batch_size: int) -> List[Path]:\n",
    "        \"\"\"Select random chunks for processing, avoiding already processed ones\"\"\"\n",
    "        # Filter out already processed chunks\n",
    "        unprocessed_chunks = [chunk for chunk in available_chunks \n",
    "                            if chunk not in self.processed_chunks]\n",
    "        \n",
    "        if not unprocessed_chunks:\n",
    "            # Reset for new pass\n",
    "            self.processed_chunks.clear()\n",
    "            unprocessed_chunks = available_chunks.copy()\n",
    "            logger.info(\"Reset processed chunks tracker for new pass\")\n",
    "        \n",
    "        # Select random batch\n",
    "        batch_size = min(batch_size, len(unprocessed_chunks))\n",
    "        selected_chunks = random.sample(unprocessed_chunks, batch_size)\n",
    "        \n",
    "        # Mark as processed\n",
    "        self.processed_chunks.update(selected_chunks)\n",
    "        \n",
    "        return selected_chunks\n",
    "        \n",
    "    def _cleanup_files(self, files: List[Path]):\n",
    "        \"\"\"Clean up temporary files\"\"\"\n",
    "        for file_path in files:\n",
    "            try:\n",
    "                if file_path.exists():\n",
    "                    file_path.unlink()\n",
    "                    logger.debug(f\"Deleted {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not delete {file_path}: {e}\")\n",
    "    \n",
    "    def _perform_shuffle_pass_parallel(self, input_chunks: List[Path], pass_number: int, \n",
    "                                      seed: int, max_workers: int = None) -> List[Path]:\n",
    "        \"\"\"Perform one complete shuffle pass with parallel batch processing\"\"\"\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"SHUFFLE PASS {pass_number} (PARALLEL)\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "        \n",
    "        # Reset processed tracker for this pass\n",
    "        self.processed_chunks.clear()\n",
    "        \n",
    "        # Estimate target chunk size for output\n",
    "        total_input_chunks = len(input_chunks)\n",
    "        \n",
    "        # Sample a chunk to estimate average rows per chunk\n",
    "        if input_chunks:\n",
    "            sample_chunk = pl.read_parquet(input_chunks[0])\n",
    "            avg_rows_per_chunk = len(sample_chunk)\n",
    "        else:\n",
    "            avg_rows_per_chunk = 10000\n",
    "        \n",
    "        # Prepare all batches upfront\n",
    "        batches = []\n",
    "        chunk_offset = 0\n",
    "        batch_number = 0\n",
    "        \n",
    "        remaining_chunks = input_chunks.copy()\n",
    "        random.shuffle(remaining_chunks)  # Randomize order\n",
    "        \n",
    "        while remaining_chunks:\n",
    "            batch_number += 1\n",
    "            batch_size = min(self.optimal_batch_size, len(remaining_chunks))\n",
    "            batch_chunks = remaining_chunks[:batch_size]\n",
    "            remaining_chunks = remaining_chunks[batch_size:]\n",
    "            \n",
    "            # Estimate output chunks for this batch\n",
    "            estimated_rows = avg_rows_per_chunk * len(batch_chunks)\n",
    "            estimated_output_chunks = math.ceil(estimated_rows / avg_rows_per_chunk)\n",
    "            \n",
    "            batches.append({\n",
    "                'chunks': batch_chunks,\n",
    "                'batch_number': batch_number,\n",
    "                'chunk_offset': chunk_offset\n",
    "            })\n",
    "            \n",
    "            # Reserve space for output chunks from this batch\n",
    "            chunk_offset += estimated_output_chunks * 2  # 2x buffer for variation\n",
    "        \n",
    "        logger.info(f\"Prepared {len(batches)} batches for parallel processing\")\n",
    "        \n",
    "        # Process batches in parallel\n",
    "        all_output_chunks = []\n",
    "        \n",
    "        if max_workers is None:\n",
    "            max_workers = min(len(batches), self.config.max_workers if hasattr(self.config, 'max_workers') else 4)\n",
    "        \n",
    "        with ProcessPoolExecutor(max_workers=max_workers, mp_context=mp.get_context('spawn')) as executor:\n",
    "            # Submit all batch jobs\n",
    "            future_to_batch = {}\n",
    "            for batch_info in batches:\n",
    "                future = executor.submit(\n",
    "                    process_batch_worker,\n",
    "                    batch_info['chunks'],\n",
    "                    pass_number,\n",
    "                    batch_info['batch_number'],\n",
    "                    seed + batch_info['batch_number'],\n",
    "                    avg_rows_per_chunk,\n",
    "                    batch_info['chunk_offset'],\n",
    "                    self.temp_dir,\n",
    "                    self.config.chunk_size_variation\n",
    "                )\n",
    "                future_to_batch[future] = batch_info['batch_number']\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_num = future_to_batch[future]\n",
    "                try:\n",
    "                    output_files, num_chunks, status = future.result()\n",
    "                    logger.info(status)\n",
    "                    all_output_chunks.extend(output_files)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Batch {batch_num} failed with exception: {e}\")\n",
    "        \n",
    "        # Clean up input chunks if they're temporary\n",
    "        if pass_number > 1:\n",
    "            self._cleanup_files(input_chunks)\n",
    "        \n",
    "        logger.info(f\"Pass {pass_number} complete: {len(all_output_chunks)} total chunks created\")\n",
    "        return all_output_chunks\n",
    "    \n",
    "    def shuffle_dataset(self, seed: int = 42, use_parallel: bool = True, \n",
    "                       max_workers: int = None) -> List[Path]:\n",
    "        \"\"\"\n",
    "        Perform complete multi-pass shuffle of the dataset.\n",
    "        \n",
    "        Args:\n",
    "            seed: Random seed for reproducibility\n",
    "            use_parallel: Whether to use parallel processing\n",
    "            max_workers: Maximum number of parallel workers (None = auto)\n",
    "        \"\"\"\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"STARTING MULTI-PASS SHUFFLE {'(PARALLEL)' if use_parallel else ''}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        logger.info(f\"Input chunks: {len(self.input_chunks)}\")\n",
    "        logger.info(f\"Required passes: {self.required_passes}\")\n",
    "        logger.info(f\"Batch size: {self.optimal_batch_size}\")\n",
    "        if use_parallel:\n",
    "            logger.info(f\"Max workers: {max_workers or 'auto'}\")\n",
    "        \n",
    "        current_chunks = self.input_chunks.copy()\n",
    "        \n",
    "        # Perform multiple shuffle passes\n",
    "        for pass_num in range(1, self.required_passes + 1):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if use_parallel:\n",
    "                current_chunks = self._perform_shuffle_pass_parallel(\n",
    "                    current_chunks, pass_num, seed * pass_num, max_workers)\n",
    "            else:\n",
    "                # Fallback to sequential processing (your original method)\n",
    "                current_chunks = self._perform_shuffle_pass(\n",
    "                    current_chunks, pass_num, seed * pass_num)\n",
    "            \n",
    "            pass_time = time.time() - start_time\n",
    "            logger.info(f\"Pass {pass_num} completed in {pass_time:.1f} seconds\")\n",
    "            \n",
    "            # Update memory stats\n",
    "            self.memory_stats = self._get_memory_info()\n",
    "            logger.info(f\"Memory usage: {self.memory_stats['percent_used']:.1f}%\")\n",
    "\n",
    "            # Test config_idx uniformity after each pass\n",
    "            tester = ConfigIdxUniformityTester(\n",
    "                config_idx_min=self.config_idx_min,\n",
    "                config_idx_max=self.config_idx_max,\n",
    "                alpha=0.05,\n",
    "            )\n",
    "            results = tester.test_chunks_uniformity(\n",
    "                [str(c) for c in current_chunks], 10000, 42)\n",
    "            tester.print_detailed_report(results)\n",
    "        \n",
    "        # Move final chunks to output directory\n",
    "        final_chunks = []\n",
    "        for i, chunk_file in enumerate(current_chunks):\n",
    "            final_path = self.output_dir / f\"chunk_{i:04d}.parquet\"\n",
    "            try:\n",
    "                chunk_file.rename(final_path)\n",
    "                final_chunks.append(final_path)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to move {chunk_file} to {final_path}: {e}\")\n",
    "\n",
    "        # Final uniformity test\n",
    "        tester = ConfigIdxUniformityTester(\n",
    "            config_idx_min=self.config_idx_min,\n",
    "            config_idx_max=self.config_idx_max,\n",
    "            alpha=0.05,\n",
    "        )\n",
    "        results = tester.test_chunks_uniformity(\n",
    "            [str(c) for c in final_chunks], 10000, 42)\n",
    "        tester.print_detailed_report(results)\n",
    "        \n",
    "        # Clean up temp directory\n",
    "        try:\n",
    "            for temp_file in self.temp_dir.glob(\"*.parquet\"):\n",
    "                temp_file.unlink()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error cleaning temp directory: {e}\")\n",
    "        \n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"SHUFFLE COMPLETE!\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        logger.info(f\"Final output: {len(final_chunks)} chunks in {self.output_dir}\")\n",
    "        \n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f81fd20-7d82-4848-b877-0f2e9e62b18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:40:35.506761Z",
     "iopub.status.busy": "2025-10-01T06:40:35.506412Z",
     "iopub.status.idle": "2025-10-01T06:40:35.512913Z",
     "shell.execute_reply": "2025-10-01T06:40:35.511908Z",
     "shell.execute_reply.started": "2025-10-01T06:40:35.506720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure shuffle parameters\n",
    "config = ShuffleConfig(\n",
    "    max_memory_usage_ratio=0.4,  # Use 80% of available memory\n",
    "    safety_margin=0.9,           # 90% safety margin\n",
    "    min_chunks_per_batch=2,\n",
    "    max_chunks_per_batch=100,\n",
    "    max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 2\n",
    "cube_size = 2**level\n",
    "\n",
    "image_source = './data/shapenet_config_ortho_vis_1_128'\n",
    "\n",
    "angle_id = 18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c654e36-959c-444b-adfb-03b31c2368f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:40:35.514908Z",
     "iopub.status.busy": "2025-10-01T06:40:35.514295Z",
     "iopub.status.idle": "2025-10-01T06:40:35.521419Z",
     "shell.execute_reply": "2025-10-01T06:40:35.520519Z",
     "shell.execute_reply.started": "2025-10-01T06:40:35.514867Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path(f'./data/chunked/level_{level}')\n",
    "\n",
    "input_data_dir = data_dir / 'chunked' / f'level_{level}'\n",
    "output_data_dir = data_dir / 'shuffled' / f'level_{level}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56906e31-d96e-4f9a-bf40-eb8548e84c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 23:10:50,296 - INFO - Found 51 input chunks\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MemoryAwareChunkShuffler' object has no attribute 'max_workers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m joined_path \u001b[38;5;129;01min\u001b[39;00m input_data_dir.glob(\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Create shuffler\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     shuffler = \u001b[43mMemoryAwareChunkShuffler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoined_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_data_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoined_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshuffle_temp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Perform shuffle\u001b[39;00m\n\u001b[32m     11\u001b[39m     final_chunks = shuffler.shuffle_dataset(seed=\u001b[32m42\u001b[39m)        \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mMemoryAwareChunkShuffler.__init__\u001b[39m\u001b[34m(self, input_dir, output_dir, temp_dir, config)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.memory_stats = \u001b[38;5;28mself\u001b[39m._get_memory_info()\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.chunk_memory_usage = \u001b[38;5;28mself\u001b[39m._estimate_chunk_memory()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.optimal_batch_size = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_optimal_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.required_passes = \u001b[38;5;28mself\u001b[39m._calculate_required_passes()\n\u001b[32m     27\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMemory available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.memory_stats[\u001b[33m'\u001b[39m\u001b[33mavailable_gb\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mMemoryAwareChunkShuffler._calculate_optimal_batch_size\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_calculate_optimal_batch_size\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     66\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Calculate optimal number of chunks to process in one batch\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     available_memory = \u001b[38;5;28mself\u001b[39m.memory_stats[\u001b[33m'\u001b[39m\u001b[33mavailable_gb\u001b[39m\u001b[33m'\u001b[39m] * \u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m / \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_workers\u001b[49m\n\u001b[32m     68\u001b[39m     usable_memory = available_memory * \u001b[38;5;28mself\u001b[39m.config.max_memory_usage_ratio\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# Calculate theoretical max chunks\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'MemoryAwareChunkShuffler' object has no attribute 'max_workers'"
     ]
    }
   ],
   "source": [
    "for joined_path in input_data_dir.glob('*'):\n",
    "    # Create shuffler\n",
    "    shuffler = MemoryAwareChunkShuffler(\n",
    "        input_dir=joined_path,\n",
    "        output_dir=output_data_dir / joined_path.name, \n",
    "        temp_dir=\"shuffle_temp\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Perform shuffle\n",
    "    final_chunks = shuffler.shuffle_dataset(seed=42)        \n",
    "\n",
    "    print(f\"{joined_path.name} Shuffle complete! Created {len(final_chunks)} shuffled chunks.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
